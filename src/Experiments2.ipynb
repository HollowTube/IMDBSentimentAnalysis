{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pos_path = \".\\\\data\\\\train\\\\pos\\\\\"\n",
    "neg_path = \".\\\\data\\\\train\\\\neg\\\\\"\n",
    "\n",
    "neg_review_list = []\n",
    "pos_review_list = []\n",
    "review_list = []\n",
    "test_set = []\n",
    "\n",
    "for file in os.listdir(neg_path):\n",
    "    file_path = os.path.join(neg_path, file)\n",
    "    fh = open(file_path, 'r', encoding=\"utf8\")\n",
    "    entry = {'class': 0, 'raw_txt': fh.read()}\n",
    "    neg_review_list.append(entry)\n",
    "    fh.close()\n",
    "    \n",
    "for file in os.listdir(pos_path):\n",
    "    file_path = os.path.join(pos_path, file)\n",
    "    fh = open(file_path, 'r', encoding=\"utf8\")\n",
    "    entry = {'class': 1, 'raw_txt': fh.read()}\n",
    "    pos_review_list.append(entry)\n",
    "    fh.close()\n",
    "\n",
    "review_list = pd.DataFrame(neg_review_list + pos_review_list)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample review of 20 for some quick checking\n",
    "sample_review = pd.DataFrame(neg_review_list[:10]+pos_review_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class                                            raw_txt  \\\n0      0  Story of a man who has unnatural feelings for ...   \n1      0  Airport '77 starts as a brand new luxury 747 p...   \n2      0  This film lacked something I couldn't put my f...   \n3      0  Sorry everyone,,, I know this is supposed to b...   \n4      0  When I was little my parents took me along to ...   \n\n                                           processed  \\\n0  story of a man who has unnatural feelings for ...   \n1  airport 77 starts as a brand new luxury 747 pl...   \n2  this film lacked something i couldnt put my fi...   \n3  sorry everyone i know this is supposed to be a...   \n4  when i was little my parents took me along to ...   \n\n                              processed_no_stopwords  sentiment  \n0  story man unnatural feelings pig starts openin...  -0.067593  \n1  airport 77 starts brand new luxury 747 plane l...   0.068553  \n2  film lacked something couldnt put finger first...   0.025000  \n3  sorry everyone know supposed art film wow hand...   0.027696  \n4  little parents took along theater see interior...  -0.084892  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from textblob import Word, TextBlob\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def text_processing(txt):\n",
    "    processed = re.sub(r'[^\\w\\s]', '', txt.lower())\n",
    "    # processed = ' '.join([Word(word).lemmatize() for word in processed.split()])\n",
    "    # processed = negationHandling(processed.split())\n",
    "    return processed\n",
    "\n",
    "def negationHandling(word_list):\n",
    "    \"\"\"\n",
    "    given an matrix of individual words in order, combine \"negation,\"word\" into\n",
    "    \"neg_word\"\n",
    "    \"\"\"\n",
    "    negation_words = (\"not\", \"hardly\", \"barely\", \"never\", \"neither\", \"scarcely\",\n",
    "                      \"doesn't\", \"doesnt\", \"isn't\", \"isnt\", \"wasn't\", \"wasnt\",\n",
    "                      \"shouldn't\", \"shouldnt\", \"wouldn't\", \"wouldnt\", \"couldn't\",\n",
    "                      \"couldnt\", \"won't\", \"wont\", \"can't\", \"cant\", \"don't\", \"dont\")\n",
    "    for index in range(len(word_list) - 1):\n",
    "        if word_list[index] in negation_words:\n",
    "            word_list[index + 1] = \"neg_\" + word_list[index + 1]\n",
    "    new_list = []\n",
    "    for word in word_list:\n",
    "        if word not in negation_words:\n",
    "            new_list.append(word)\n",
    "    return ' '.join(new_list)\n",
    "\n",
    "def processing(df):\n",
    "    # lowering, removing punctuation and lemmatization\n",
    "    df['processed'] = df['raw_txt'].apply(lambda x: text_processing(x))\n",
    "\n",
    "    # Removing stopwords    \n",
    "    df['processed_no_stopwords'] = df['processed'].apply(lambda x: ' '.join([t for t in x.split(' ')if t not in stopWords]))\n",
    "                                        \n",
    "    # Sentiment\n",
    "    df['sentiment'] = df['processed_no_stopwords'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "    \n",
    "    return df\n",
    "\n",
    "sample = processing(sample_review)\n",
    "df = processing(review_list)\n",
    "print(sample.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 raw_txt  \\\n5610   Devil Hunter gained notoriety for the fact tha...   \n3742   What can I say about Seven Pounds...well I wat...   \n5692   This film was so predictable, that during the ...   \n22213  In an attempt to cash in on the success of Uni...   \n23165  Dark comedy? Gallows humor? How does one make ...   \n\n                                               processed  \\\n5610   devil hunter gained notoriety for the fact tha...   \n3742   what can i say about seven poundswell i watche...   \n5692   this film was so predictable that during the e...   \n22213  in an attempt to cash in on the success of uni...   \n23165  dark comedy gallows humor how does one make a ...   \n\n                                  processed_no_stopwords  sentiment  \n5610   devil hunter gained notoriety fact dpp video n...   0.007029  \n3742   say seven poundswell watched flight seattle to...  -0.176136  \n5692   film predictable entire time youre hoping obvi...   0.036243  \n22213  attempt cash success universals horror films m...   0.142929  \n23165  dark comedy gallows humor one make comedy murd...   0.200376  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting up the data set\n",
    "features = [f for f in df.columns.values if f not in ['class']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[features], df['class'],\n",
    "                                                    test_size=0.20, random_state=10)\n",
    "x_set = df[features]\n",
    "target_set = df['class']\n",
    "print(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import GenericUnivariateSelect,mutual_info_classif,SelectPercentile,f_classif\n",
    "from sklearn.linear_model import Lasso,Ridge\n",
    "\n",
    "sentiment = Pipeline([\n",
    "    ('selector', Selector(key='sentiment'))\n",
    "])\n",
    "\n",
    "tf_id_feature = Pipeline([\n",
    "    ('selector', Selector(key='processed_no_stopwords')),\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 2), max_features=100000))\n",
    "    #('feature_selection', SelectPercentile(score_func=mutual_info_classif, percentile=50))\n",
    "])\n",
    "\n",
    "binary_count_feature = Pipeline([\n",
    "    ('selector', Selector(key='processed_no_stopwords')),            \n",
    "    # ('count', CountVectorizer(ngram_range=(1, 2), max_features=100000)),\n",
    "    ('count', CountVectorizer())\n",
    "    # ('feature_selection', SelectPercentile(score_func=f_classif, percentile=50))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feats_tfid = FeatureUnion([('text', tf_id_feature),\n",
    "                           ])\n",
    "\n",
    "feats_count = FeatureUnion([('text', binary_count_feature),\n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def final_features(features_pipeline):\n",
    "    features_pipeline.fit(x_train, y_train)\n",
    "    return features_pipeline.transform(x_train), features_pipeline.transform(x_test)\n",
    "\n",
    "\n",
    "x_train_final, x_test_final = final_features(feats_tfid)\n",
    "print(x_train_final.shape)\n",
    "print(x_test_final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tri-Tin\\Anaconda3\\envs\\LinearRegression\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tri-Tin\\Anaconda3\\envs\\LinearRegression\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7202\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "svc_pipeline = Pipeline([\n",
    "    ('feats', tf_id_feature),\n",
    "    ('classifier', LinearSVC())\n",
    "])\n",
    "svc_pipeline_count = Pipeline([\n",
    "    ('feats', binary_count_feature),\n",
    "    ('classifier', LinearSVC())\n",
    "])\n",
    "\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('feats', tf_id_feature),\n",
    "    ('voting', VotingClassifier(estimators=[('svc', LinearSVC()),\n",
    "                                            ('mnb', MultinomialNB()),\n",
    "                                            ('lr', LogisticRegression(C=0.5))],voting='hard'))       \n",
    "])\n",
    "\n",
    "logistic_pipeline = Pipeline([\n",
    "    ('feats', binary_count_feature),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "decision_tree_pipeline = Pipeline([\n",
    "    ('feats', binary_count_feature),\n",
    "    ('classifier', tree.DecisionTreeClassifier())])\n",
    "\n",
    "naive_bayes_pipeline = Pipeline([\n",
    "    ('feats', binary_count_feature),\n",
    "    ('classifier', MultinomialNB())])\n",
    "\n",
    "lasso = Pipeline([\n",
    "    ('feats', tf_id_feature),\n",
    "    ('classifier', Lasso(alpha=0.3,fit_intercept=True))])\n",
    "\n",
    "ridge = Pipeline([\n",
    "    ('feats', tf_id_feature),\n",
    "    ('classifier', Ridge(alpha=10, fit_intercept=True))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tri-Tin\\Anaconda3\\envs\\LinearRegression\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tri-Tin\\Anaconda3\\envs\\LinearRegression\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7202\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fit_predict(model):\n",
    "    model.fit(x_train, y_train)\n",
    "    print(model.score(x_test, y_test))\n",
    "\n",
    "print(\"running\")\n",
    "fit_predict(svc_pipeline_count)\n",
    "fit_predict(logistic_pipeline)\n",
    "fit_predict(naive_bayes_pipeline)\n",
    "fit_predict(decision_tree_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tri-Tin\\Anaconda3\\envs\\LinearRegression\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_svc_tfidf = cross_val_score(svc_pipeline, x_set, target_set, cv=4)\n",
    "scores_logistic = cross_val_score(logistic_pipeline, x_set, target_set, cv=4)\n",
    "scores_NB = cross_val_score(naive_bayes_pipeline, x_set, target_set, cv=4)\n",
    "scores_DT = cross_val_score(decision_tree_pipeline, x_set, target_set, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.854 (+/- 0.02)\nAccuracy: 0.842 (+/- 0.02)\nAccuracy: 0.788 (+/- 0.01)\nAccuracy: 0.716 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (scores_svc_tfidf.mean(), scores_svc_tfidf.std() * 2))\n",
    "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (scores_logistic.mean(), scores_logistic.std() * 2))\n",
    "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (scores_NB.mean(), scores_NB.std() * 2))\n",
    "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (scores_DT.mean(), scores_DT.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             raw_txt\n0  Thinking that it could only get better was the...\n1  For most people, RoboCop 3 is the film that re...\n2  I'm pretty sure Poe would have considered this...\n3  This is one of those made-for-TV B movies that...\n4  Wallace & Gromit have been around for some tim...\n(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "test_path = \".\\\\data\\\\test\\\\\"\n",
    "test_set = []\n",
    "\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "dir_list = sorted(os.listdir(test_path), key=numericalSort)\n",
    "\n",
    "for file in dir_list:\n",
    "    file_path = os.path.join(test_path, file)\n",
    "    fh = open(file_path, 'r', encoding=\"utf8\")\n",
    "    entry = {'raw_txt': fh.read()}\n",
    "    test_set.append(entry)\n",
    "    fh.close()\n",
    "\n",
    "test_set = pd.DataFrame(test_set)\n",
    "print(test_set.head())\n",
    "print(test_set.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             raw_txt  \\\n0  Thinking that it could only get better was the...   \n1  For most people, RoboCop 3 is the film that re...   \n2  I'm pretty sure Poe would have considered this...   \n3  This is one of those made-for-TV B movies that...   \n4  Wallace & Gromit have been around for some tim...   \n\n                                           processed  \\\n0  thinking that it could only get better wa the ...   \n1  for most people robocop 3 is the film that rea...   \n2  im pretty sure poe would have considered this ...   \n3  this is one of those madefortv b movie that is...   \n4  wallace gromit have been around for some time ...   \n\n                              processed_no_stopwords  sentiment  \n0  thinking could get better wa worst assumption ...   0.151250  \n1  people robocop 3 film really big disgrace robo...   0.060466  \n2  im pretty sure poe would considered travesty f...   0.009722  \n3  one madefortv b movie awful kind endearsbr br ...  -0.110317  \n4  wallace gromit around time wa first foray onto...   0.258611  \n   Category\n0         0\n1         0\n2         0\n3         0\n4         1\n"
     ]
    }
   ],
   "source": [
    "test_set = processing(test_set)\n",
    "svc_pipeline.fit(x_train, y_train)\n",
    "test_predictions = svc_pipeline.predict(test_set)\n",
    "\n",
    "prediction_df = pd.DataFrame(test_predictions)\n",
    "\n",
    "prediction_df.columns = ['Category']\n",
    "print(test_set.head())\n",
    "print(prediction_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = prediction_df.to_csv (r'.\\test_results.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
